{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Journey Analytics in E-commerce\n",
    "Mapping the customer decision journey has become crusial tool to help companies to win customers. According to a global survey of over 7,000 marketers and e-commerce professionals, 24% of respondents ranked the customer experience as the single most exciting opportunity ahead. And, 78% agree that the customer experience creates brand differentiation and competitive advantage.\n",
    "\n",
    "The sucessfull journey mapping requires healthy balance of qualitative and quantitative information on which organization can act uppon on. While qualitative information is gathered based on experience of customer facing fuctions, the role of data scientist comes handy when supporting marketing with quantitative knowledge.\n",
    "\n",
    "## 1. Business Understanding\n",
    "The purpose of this project is to share with fellow data scientists how they can support marketing and sales with quantitative analysis during customer journey mapping. What kind of ML algoritm to use to asnwer different marketing & sales questions like:\n",
    " 1. How many buyer personas do we have?\n",
    " 2. What are their unique characterstics?\n",
    " 3. What is the flow in their journey?\n",
    " 4. What are their main painpoints?\n",
    " 5. How we can increase their engagment to our brand?\n",
    " \n",
    "### 1.1 Buyer Personas\n",
    "A buyer persona is a research-based representation of customer. It includes \n",
    " - demographics, such as age, gender, or geography\n",
    " - attitudes\n",
    " - behaviors\n",
    " - motivations \n",
    " \n",
    "It describes who the customer is, their goals, their concerns, and how they think. It includes the decision criteria needed to addressed to win their business, such as how they \n",
    " - receive information\n",
    " - weigh and evaluate different options\n",
    " - when and where they decide to buy \n",
    " \n",
    "So, basically a buyer persona is a profile of the customers who need to be engaged throughout their decision journey to drive their loyalty and advocacy as they choose comany brand over the competition. Buyer personas help better understand customers so company can choose the right marketing touchpoints and messaging, and this will attract more social shares and increase brand rankings in search engines\n",
    "\n",
    "### 1.2 Customer Journey\n",
    "A customer ourney shows the whole customer experience from initial brand awareness through purchase, and importantly after the purchase to reach customers at the moments that most influence their decisions.\n",
    "\n",
    "These insights reveal which marketing touchpoints to use, such as search, a website, online banners, or a magazine ad, and guide what kind of messaging to communicate in each of those touchpoints to engage your customers. In that way resources an be focused in the right places and right information can be shared to customers.\n",
    "\n",
    "As scope is quite big I will narrow it from intial brand awareness to purchase.\n",
    "\n",
    "### 1.3 E-commerce Use Case\n",
    "As most of the companies keeps data related to customer journey confidential, the use case will be demostrated on [Google Analytics Sample Dataset](https://support.google.com/analytics/answer/7586738?hl=en)\n",
    "\n",
    "The sample dataset contains obfuscated Google Analytics 360 data from the [Google Merchandise Store](https://shop.googlemerchandisestore.com/), a real ecommerce store. The Google Merchandise Store sells Google branded merchandise. The data is typical of what you would see for an ecommerce website. It includes the following kinds of information:\n",
    "\n",
    " - __Traffic source__ data: information about where website visitors originate. This includes data about organic traffic, paid search traffic, display traffic, etc.\n",
    " - __Content__ data: information about the behavior of users on the site. This includes the URLs of pages that visitors look at, how they interact with content, etc.\n",
    " - __Transactional__ data: information about the transactions that occur on the Google Merchandise Store website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# -------\n",
    "\n",
    "# Standard libraries\n",
    "from importlib import reload\n",
    "import pathlib\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import itertools\n",
    "import ipdb\n",
    "import string\n",
    "import re\n",
    "\n",
    "# 3rd party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)\n",
    "\n",
    "import nltk\n",
    "# nltk.download(['wordnet', 'stopwords'])\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Local libraries\n",
    "from bigquery_ import BigqueryTable\n",
    "from bigquery_ import BigqueryDataset\n",
    "import helper\n",
    "\n",
    "\n",
    "# AUTHENTIFICATION & ACCESS\n",
    "# -------------------------\n",
    "\n",
    "# Supply your service account key\n",
    "service_account = pathlib.Path(\n",
    "    ('C:/Users/Fredo/Google Drive/Knowledge Center'\n",
    "    '/Data Scientist Nanodegree/customer-journey-31622634430d.json')\n",
    ")\n",
    "\n",
    "# Initiate biqquery client\n",
    "client = bigquery.Client.from_service_account_json(service_account.absolute())\n",
    "\n",
    "# Authenticate service account\n",
    "# Authenticate_service_account(service_account_key)\n",
    "\n",
    "# Get dataset reference\n",
    "project = 'bigquery-public-data'\n",
    "dataset_id = 'google_analytics_sample'\n",
    "dataset_ref = client.dataset(dataset_id, project=project)\n",
    "dataset = BigqueryDataset(client, dataset_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "Data can be accessesed via BigQuery. They are stored in form of daily tables from 1st Aug 2016 to 1st Aug 2017. As the dataset schema does not reveal any fields description, it needs to be merged with [BigQuery Export Schema](https://support.google.com/analytics/answer/3437719?hl=en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate dataset schema\n",
    "display(dataset.schema.shape)\n",
    "display(dataset.schema['table_name'].head(n=3))\n",
    "display(dataset.schema['table_name'].tail(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Investigate table schema\n",
    "# Load BigQuery export schema\n",
    "bq_exp_schema = pd.read_excel('google_analytics_schema.xlsx', \n",
    "                              sheet_name='bq_exp_schema')\n",
    "\n",
    "# Load google analytics sample table\n",
    "table_id = 'ga_sessions_20170801'\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Recast table to custom BigqueryTable class\n",
    "table.__class__ = BigqueryTable\n",
    "\n",
    "# Load google analytics schema and merge it with export schema\n",
    "# to get field descriptions\n",
    "ga_schema = table.schema_to_dataframe(bq_exp_schema)\n",
    "table.display_schema(ga_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of variables, excluding records\n",
    "ga_schema[ga_schema['Data Type'] != 'RECORD'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyse variable levels\n",
    "# -----------------------\n",
    "# Running this code will take 18h on Intel(R) i5 2.2GHz, RAM 8GB,\n",
    "\n",
    "# Update schema by variable level characteristics\n",
    "# ga_schema = dataset.get_levels(client, bq_exp_schema)\n",
    "\n",
    "# Save updated schema with levels to HDF5 file\n",
    "# ga_schema.to_hdf('temp_data.h5', 'schema', mode='w', table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load updated schema with levels from HDF5 file\n",
    "ga_schema = pd.read_hdf('temp_data.h5', 'schema')\n",
    "\n",
    "# Print variable levels overview\n",
    "schema_levels = (ga_schema\n",
    "                 .groupby(['Num of Levels'])\n",
    "                 .agg({'Field Name': 'count', \n",
    "                       'Levels': lambda level: set().union(*level.to_list())})\n",
    "                 .rename({'Field Name': 'Num of Variables'})\n",
    "                 .sort_index())\n",
    "table.display_schema(schema_levels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select variables and analyze scales\n",
    "# ----------------------------------\n",
    "\n",
    "# Discard variables with 1 level\n",
    "schema_multi_levels = (\n",
    "    ga_schema[(ga_schema['Num of Levels'] > 1) \n",
    "              | (ga_schema['Field Name'] == 'totals.visits')]\n",
    "    .set_index('Field Name')\n",
    ")\n",
    "table.display_schema(schema_multi_levels.drop(columns=['Levels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discard not related or too detailed and too general variables \n",
    "# (done manually in excell file)\n",
    "schema = (pd.read_excel('google_analytics_schema.xlsx', \n",
    "                         sheet_name='ga_schema_uni_level_excl')\n",
    "          .set_index('Variable Name'))\n",
    "display(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable selection, Scale and personas characteristics summary\n",
    "for column in ['Status', 'Scale', 'Field Group']:\n",
    "    display(schema.groupby(column)[column].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only selected and considered variables summary\n",
    "for column in ['Status', 'Scale', 'Field Group']:\n",
    "    display(schema[(schema['Status'] == 'CONSIDER')\n",
    "                   | (schema['Status'] == 'SELECTED')]\n",
    "            .groupby(column)[column]\n",
    "            .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Understanding Discussion\n",
    "There are 366 tables in the dataset. Each table represent one day from 1 Aug 2016 to 1 Aug 2017. Each table row represents one [session](https://support.google.com/analytics/answer/2731565?hl=en) and includes [nested and repeated fields](https://www.kaggle.com/alexisbcook/nested-and-repeated-data), which expands granularity of data to following levels\n",
    "- `session`\n",
    "    - `customDimesions`\n",
    "    - `hits`\n",
    "        - `customDimensions`\n",
    "        - `products`\n",
    "            - `customDimensions`\n",
    "            - `customMetrics`\n",
    "        - `promotions`\n",
    "        - `experiments`\n",
    "        - `customVariables`\n",
    "        - `customDimensions`\n",
    "        - `customMetrics`\n",
    "        - `publisher_info`\n",
    "\n",
    "Totaly there is 306 variables where 182 have only 1 level/code. These are missing data columns, columns not available in demo dataset or constant columns not giving value to analysis objectives. All of them will be discarded except of `totals.visits` for aggregation purposes.\n",
    "\n",
    "Further more 65 Variables has been excluded for following reasons\n",
    "- not related for characteristics of buyer personas\n",
    "- too detailed considering practical description of buyer personas\n",
    "- too general not giving enough differenciation of buyer personas\n",
    "\n",
    "Remaining 60 variables were split to two categories\n",
    "- 34 selected just could discribe personas and give practical meaningful information\n",
    "- 26 considered to be used later in case personas description is not detial enough\n",
    "\n",
    "Out of 60 variables 40 are nominal, 12 numeric, 3 binary, 3 ordinal, 1 datetime stamp, 1 day stamp. Out of 60 48 are behavior, 9 demographics and 3 attitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "To help marketing team to answer how many buyer personas are there and what are their characterstics, I will develope taxonomy of the customers based on their purchase behavior such as:\n",
    " - How much do they spend?\n",
    " - How frequently do they buy?\n",
    " - When they made first purchase?\n",
    " - When they made most recent purchase?\n",
    "\n",
    "them I will look to other customer characteristics and create profile for each taxonomy group (segment, buyer persona).\n",
    "\n",
    "The primary objective is to develope a taxonomy that segments customers into groups with similar purhase behaviors. Ones identified, marketing strategies with different appeals can be fomulated for each group. The customer segmenation method used is called [RFM](https://link.springer.com/article/10.1057/dbm.2012.17) and following transaction data are needed:\n",
    " - time\n",
    " - transaction_id\n",
    " - transaction_revenue\n",
    " - client_id\n",
    "\n",
    "to be aggregated on customer level and calculate:\n",
    " - `first_purchase`: days after 1st customer purchase\n",
    " - `recency`: days after last customer purchase\n",
    " - `frequency`: # of the customer purchases\n",
    " - `monetary`: # total revenue of customer purchases\n",
    " \n",
    "From ML perspective clustering algorithm will be used. Outliers and multicoliearity need to be checked as clustering solutions are sensitive to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Query selected variables from google analytics dataset\n",
    "query = '''\n",
    "SELECT\n",
    "    date,\n",
    "    hits.transaction.transactionId AS transaction_id,\n",
    "    fullVisitorId AS client_id,\n",
    "    (hits.transaction.transactionRevenue / 1e6) AS revenue\n",
    "FROM\n",
    "    `bigquery-public-data.google_analytics_sample.ga_sessions_*`,\n",
    "    UNNEST(hits) AS hits\n",
    "WHERE\n",
    "    _TABLE_SUFFIX BETWEEN @start_date AND @end_date\n",
    "    AND hits.transaction.transactionRevenue IS NOT NULL\n",
    "ORDER BY\n",
    "    date\n",
    "'''\n",
    "\n",
    "query_params = [\n",
    "    bigquery.ScalarQueryParameter(\n",
    "        'start_date', 'STRING', \n",
    "        dataset.schema['table_name'].values[0].split('_')[-1]),\n",
    "    bigquery.ScalarQueryParameter(\n",
    "        'end_date', 'STRING', dataset.schema['table_name'].values[-2].split('_')[-1])\n",
    "]\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.query_parameters = query_params\n",
    "df = client.query(query, job_config=job_config).to_dataframe()\n",
    "\n",
    "# Cast variables\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['client_id'] = df['client_id'].astype(str)\n",
    "\n",
    "# Prints dataframe charactersitics\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "display(df.describe(include=np.number))\n",
    "display(df.describe(exclude=np.number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate RFM variables\n",
    "present = df['date'].max()\n",
    "rfm = df.groupby('client_id').agg({\n",
    "    'date': {'first_purchase':lambda date: (present - date.min()).days,\n",
    "             'recency': lambda date: (present - date.max()).days},\n",
    "    'transaction_id': {'frequency':  lambda id_: len(id_)},\n",
    "    'revenue': {'monetary': lambda revenue: revenue.sum()}})\n",
    "rfm.columns = rfm.columns.droplevel()\n",
    "\n",
    "display(rfm.head())\n",
    "display(rfm.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions log scale on y-axis of diagonal historgrams\n",
    "sns.pairplot(rfm); # diag_kws=dict(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply log transformation to exponential and chi-squared distributions\n",
    "rfm[['ln_frequency', 'ln_monetary']] = np.log1p(rfm[['frequency', 'monetary']])\n",
    "sns.pairplot(rfm[['frequency', 'ln_frequency', 'monetary', 'ln_monetary']],\n",
    "             diag_kws=dict(log=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate multicolinearity\n",
    "sns.heatmap(rfm.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate oultiers based on multivariate dissimilarity\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Create original and transformed variable sets\n",
    "idxs = {'original': pd.Index(['first_purchase', 'recency', \n",
    "                              'frequency', 'monetary']),\n",
    "        'transformed': pd.Index(['first_purchase', 'recency', \n",
    "                                 'ln_frequency', 'ln_monetary'])}\n",
    "\n",
    "# Calculate dissimilarity for both sets\n",
    "dissimilarity = pd.DataFrame()\n",
    "for idx_name, idx in idxs.items():\n",
    "    dissimilarity[idx_name] = helper.get_dissimilarity(rfm[idx])\n",
    "\n",
    "# Print distributions of dissimilarities\n",
    "axes = dissimilarity.hist(log=True)\n",
    "plt.suptitle('Distribution of Customers Dissimilarity')\n",
    "for ax in axes[0]:\n",
    "    ax.set_xlabel('Dissimilarity')\n",
    "axes[0, 0].set_ylabel('Log # of customers')\n",
    "plt.show()\n",
    "\n",
    "# Display overview\n",
    "ext_summary = (pd.concat([rfm[idxs['original']], dissimilarity], \n",
    "                             axis=1)\n",
    "                 .sort_values('transformed', ascending=False))\n",
    "display(ext_summary[:10])\n",
    "display(dissimilarity.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monetary value of outliers\n",
    "display((ext_summary['monetary'][1].sum() / rfm['monetary'].sum()).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preparation Discussion\n",
    "There are 12028 transactions, 9962 customers and no missing values in dataset. \n",
    "\n",
    "It seems that google merchendise store:\n",
    "- is slightly growing new customers from serious drop-out of 200 days ago (present means 1st Aug 2017)\n",
    "- was doing extremely well in number of customers purchasing goods 220-240 days ago\n",
    "- but is not doing well keeping customers as >75% of them purchased only ones\n",
    "\n",
    "#### 3.1.1 Multicollinearity\n",
    "`first_purhase` and `recency` are higly collinear which inflates importance of time dimension over frequency and monetary value and can negatively effects forming of clusters. I decided to exclude `first_purchase` from analysis.\n",
    "\n",
    "#### 3.1.2 Distributions\n",
    "`first_purchase` and `recency` are uniformly or multimodal distributed.`frequency` approximate expenential and `monetary` chi-square distribution. As these are highly skewed distributions both of variables are transformed using natural logarithm: $x_{tr} = ln(x+1)$. Transformation will reduce number of outliers and enable clustering algorithm to form more homogenous clusters with more equal sizes.\n",
    "\n",
    "#### 3.1.3 Outliers\n",
    "Multivariate dissimilarity has been used to identify outlier customers from average customer. While raw data shows 1 outlier with huge customer monetary value of 128K$. Even if this is outlier it need to be kept in dataset as it represents 7.2% of total revenue.\n",
    "\n",
    "Using transformation shows no outliers but 8 underpresented customers in the population, which need to be kept in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "As there is no conceptual knowledge of number of the buyer personas, I will hold to practical consideration of having 3-7 clusters, suppose that distict marketing strategy will be developed for each of the buyer persona. First hierarchical clustering will be used in order to examine all possible cluster solutions. The candidate solutions will be selected for non-hierachical algorithm to form more compact clusters. Then final solution from candidates will be selected in respect to practical consideration.\n",
    "\n",
    "### 4.1 Research design of the cluster analysis\n",
    "\n",
    "#### 4.1.1 Define similarity measure\n",
    "Given that all tree clustering variable are metric the __Euclidean Distance__ is chosen as the similarity measure. If `first_purchase` would need to be included from business point of view, than __Mahalanobis Distance__ could be used to deal with multicolinearity issue.\n",
    "\n",
    "#### 4.1.2  Sample size\n",
    "As customer sample is quite big (9962 customers) we can split sample by half to cross-validate cluster solution. Random split can be used with conditions that underepresented customers will remain in both samples to keep data structure. \n",
    "\n",
    "#### 4.1.3  Standardization\n",
    "As the averages and standard deviations of the variables vary a lot, they need to be standardized before entering to cluster algorithm in order to ensure assigning equal weights to each variable when forming clusters.\n",
    "\n",
    "### 4.2 Assumptions in cluster analysis\n",
    "#### 4.2.1 Sample representativeness\n",
    "Considering objective of identifying buyer personas, whole customer population (population = all customer transactions during existance of Google Merchendise Shop) or random sample from that population should be taken. Unfortunately this was not done as available sample is related to fixed time period from 1 Aug 2016 to 1 Aug 2017. For the purpose of this excercise I will limit populaton to this time period. When population is randomly split to half, both of samples can be considered representative.\n",
    "\n",
    "#### 4.2.2 Multicollinearity\n",
    "Multicollinearity effects are minimized trough the variable selection process, where `first_purchase` variable was excluded from dataset.\n",
    "\n",
    "### 4.3 Selecting clustering algorithm\n",
    "- __Aglomerative Clustering__ will be used in the first step, due to its ability to generate all clustering solutions, time efficiency to handle big samples. The __ward__ method will be used based on __Squared Eclidian Distance__ with advantage of forming homogenous clusters with relatively equal in size. This is especially efficient to used dataset where clustering variable distributions are hihgly skewed with outliers.\n",
    "- __K-Nearest Neighbour__ non-hierarchical algorithm will be used, due to its ability to handle large samples especialy __MiniBatch__ version  in second step to form more compact clusters\n",
    "\n",
    "### 4.3.1 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split data into two sets, and keep outliers in each set\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Make results deterministic\n",
    "random_state=1\n",
    "\n",
    "# Select variables\n",
    "X = rfm.drop(columns=['first_purchase'])\n",
    "for idx_name, idx in idxs.items():\n",
    "    idxs[idx_name] = idx.drop('first_purchase', errors='ignore')\n",
    "\n",
    "# Split datasets\n",
    "threshold = 10\n",
    "X_names = ['train', 'test']\n",
    "size = 0.5\n",
    "Xs = helper.split_data(X, idxs['transformed'], X_names, threshold, \n",
    "                       test_size=size, random_state=random_state)\n",
    "\n",
    "for X_name, X in Xs.items():\n",
    "    print('Dataset', X_name, 'of shape', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD AND OPTIMIZE MODEL FOR 1st DATASET\n",
    "# ----------------------------------------\n",
    "\n",
    "# Standardize datasets\n",
    "scalers={}\n",
    "Xs_std = {}\n",
    "idxs['all_inputs'] = pd.Index([\n",
    "    'recency', 'frequency', 'monetary', 'ln_frequency', 'ln_monetary', \n",
    "    'recency_std', 'frequency_std', 'monetary_std','ln_frequency_std', \n",
    "    'ln_monetary_std'\n",
    "])\n",
    "\n",
    "for X_name, X in Xs.items():\n",
    "    scalers[X_name] = StandardScaler().fit(X)\n",
    "    Xs_std[X_name] = scalers['train'].transform(X)\n",
    "    Xs_std[X_name] = pd.DataFrame(Xs_std[X_name], \n",
    "                                  columns=Xs[X_name].columns+'_std', \n",
    "                                  index=Xs[X_name].index)\n",
    "    Xs[X_name] = (pd.concat([Xs[X_name], Xs_std[X_name]], axis=1)\n",
    "                  .reindex(idxs['all_inputs'], axis=1))\n",
    "\n",
    "# Agglomerative Clustering with average linkage\n",
    "idxs['transformed_std'] = pd.Index(\n",
    "    ['recency_std', 'ln_frequency_std', 'ln_monetary_std'])\n",
    "\n",
    "agl_cluster = linkage(Xs_std['train'][idxs['transformed_std']], 'ward')\n",
    "\n",
    "# Construct dendrogram\n",
    "plt.figure(figsize=(6, 6))\n",
    "dn = dendrogram(agl_cluster, p=30, truncate_mode='lastp', orientation='right')\n",
    "plt.title('Customers Dendrogram')\n",
    "plt.xlabel('Aglomeration Coeficient')\n",
    "plt.ylabel('Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Inspect outliers in agglomeration schedule if any\n",
    "pass\n",
    "\n",
    "# Step 2: Consider removing outliers if appropriate\n",
    "pass\n",
    "\n",
    "# Step 3: Run algomerative clustering again go to step 1\n",
    "pass\n",
    "\n",
    "# Note: no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Determine number of candidate solutions\n",
    "# ---------------------------------------\n",
    "\n",
    "# Get aglomeration shedule\n",
    "agl_schedule = pd.DataFrame(\n",
    "    agl_cluster,\n",
    "    columns=['cluster1', 'cluster2', 'coefficient', 'cluster_size'],\n",
    "    index=pd.Index(range(1, Xs_std['train'].shape[0]), name='stage')\n",
    ")\n",
    "\n",
    "# Calculate proportional heterogeneity increase\n",
    "agl_schedule['num_of_clusters'] = list(reversed(agl_schedule.index))\n",
    "agl_schedule['prop_heterogeneity_increase'] = (\n",
    "    agl_schedule['coefficient']\n",
    "    .diff()\n",
    "    .shift(periods=-1)\n",
    "    / agl_schedule['coefficient']\n",
    ")\n",
    "\n",
    "# Print last x stages of algomeration schedule\n",
    "stages = 15\n",
    "display(agl_schedule[-stages:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print heterogenity scree plot\n",
    "(agl_schedule\n",
    " .set_index('num_of_clusters')\n",
    " ['prop_heterogeneity_increase'][-stages:]\n",
    " .plot(title='Heterogeneity Scree Plot'))\n",
    "plt.ylabel('Proporion heterogeneity increase to next stage');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected candidate solutions\n",
    "num_seeds = [3, 6]\n",
    "\n",
    "# Profile candiate solutions\n",
    "# --------------------------\n",
    "\n",
    "for k in num_seeds:\n",
    "    \n",
    "    # Fenerate candidate solution labels\n",
    "    labels = fcluster(agl_cluster, k, criterion='maxclust')\n",
    "    Xs['train']['hierarchic_clusters_{}'.format(k)] = labels\n",
    "    \n",
    "    # Display cluster sizes\n",
    "    display(Xs['train'].groupby('hierarchic_clusters_{}'.format(k))\n",
    "                       .size()\n",
    "                       .rename('cluster_sizes'))\n",
    "\n",
    "display(Xs['train'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test candidate solutions\n",
    "# ---------------------------\n",
    "\n",
    "# Initiate profile function arguments\n",
    "idxs['hierarchic_clusters'] = Xs['train'].columns[\n",
    "    Xs['train'].columns.str.contains('hierarchic_clusters')]\n",
    "\n",
    "post_hoc_test = sp.posthoc_nemenyi\n",
    "\n",
    "# Execute cluster profile tests, print cluster profiles & scatterplots\n",
    "for solution in idxs['hierarchic_clusters']:\n",
    "    summary, post_hoc, prof_ax, clst_pg = (\n",
    "        helper.analyze_cluster_solution(\n",
    "            Xs['train'], idxs['transformed_std'], \n",
    "            solution, post_hoc_fnc=post_hoc_test\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Print original variable means and averages per cluster\n",
    "    str_ = 'Profile characteristics'\n",
    "    print(str_ + '\\n' + '-' * len(str_))\n",
    "\n",
    "    display(Xs['train']\n",
    "            .groupby(solution)\n",
    "            [idxs['original']]\n",
    "            .agg(['mean', 'median'])\n",
    "            .round(1)\n",
    "            .swaplevel(axis=1)\n",
    "            .sort_index(level=0, axis=1))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue overview of 6 cluster solution\n",
    "monetary_cls_6 = (Xs['train']\n",
    "                  .groupby('hierarchic_clusters_6', as_index=False)\n",
    "                  ['monetary']\n",
    "                  .sum())\n",
    "monetary_cls_6['prop_monetary'] = (monetary_cls_6['monetary'] \n",
    "                                   / Xs['train']['monetary'].sum())\n",
    "display(monetary_cls_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.1 Hierarchical Clustering Discussion\n",
    "##### 4.3.1.1.1 Initial Cluster Results\n",
    "Dendrogram and aglomeration schedule helps to understnand the clustering process. \n",
    "\n",
    "Dendogram shows that indeed natural logarithm tranformation of `recency` and `monetary` and __ward method__ have helped to form relatively equal clusters with no outliers. When inspecting solution with 30 clusters the cluster size ranges from 13 to 439customers. There are no clusters with single customer which shows alorithm robustness towards outliers.\n",
    "\n",
    "Ex. if __Average Linkage__ method would be used there would be tree clusters [4977, 4983, 4980] with one customer in later algomeration stages. It would be necessary to remove those and re-run the algorithm again to reach similar cluster homogenity and sizes as in __Ward Method__ case.\n",
    "\n",
    "_Note:_ To try __Average Linkage__ method just change `ward` to `average` in cell In [18]\n",
    "\n",
    "##### 4.3.1.1.2 Determining Preliminary Cluster Solutions\n",
    "Candidate cluster solutions will be specified using __Stopping Rule__. It is based on assesing the changes in heterogeity between cluster solutions. The basic rational is that when large change in heterogeneity occur in moving form one stage to the next, the prior cluster solutions should be selected because the next stage is joining quite diffrent clusters. The proportion heterogeneity change to next stage is calculated as: \n",
    "\n",
    "$prop\\_heterogeneity\\_increase_{\\text{i}} = \\cfrac{coefficient_{\\text{i+1}} - coefficient_{\\text{i}}}{coefficient_{\\text{i}}}$\n",
    "\n",
    "__Heterogeneity Scree Plot__ shows that proprotionaly highest heterogeneity increase is when going from 3 to 2 and 6 to 5 clusters solutions. Therefore two solution candidates with 3 and 6 clusters were further selected for profiling.\n",
    "\n",
    "##### 4.3.1.1.3 Profiling of Cluster Solutions\n",
    "Before proceeding to nonhierarchical analysis, the profiling of selected cluster solutions is done to confirm that the differences between customer clusters are distincitive and sifnificant in recency and frequency of purchase and revenue (clustering variables)\n",
    "\n",
    "Using __one-way ANOVA__ to identify clustering variable is not practical in this case as ANOVA prerequisites of __residuals normality__ (Shapiro-Wilk Test) and __group variance homogeneity__ (Leven's test) are not met. Instead non parametric __Kruskal-Wallis test__ was used to evaluate overal sifnificance and __Post-hoc Nemenyi__ was used to evaluate pair-wise significance.\n",
    "\n",
    "The results shows that there are significant diffrences between the clusters on all tree variables in both solutions. These provide initial evidence that each of the tree and six clusters is distinctive.\n",
    "\n",
    "Examination of means of cluster variables shows:\n",
    " - tree clusters solution\n",
    "   - cluster 1 (609 customers) has highest mean on frequency 3.9 and monetary value 867USD representing ___\"frequent shopers and high spenders\"___\n",
    "   - cluster 2 (2171 customers) with highest recency 271 days, low frequency 1, most probably representing ___\"customer churn\"___\n",
    "   - cluster 3 (2204 customers) with lowest recency 94 days, low frequency 1, represening ___\"new customers\"___ \n",
    "   \n",
    " - six clusters solution\n",
    "   - cluster 1 (83 customers) has highest mean on frequency 7.7 and monetary value 4162USD representing ___\"frequent shoppers and huge spenders\"___ most probably business customers\n",
    "   - cluster 2 (526 customers) has second highest mean on frequency 2.3 (no significant diffrence with cluster 1 frequency mean 7.7) and second highest monetary value 347USD representing ___\"frequent shoppers and moderate spenders\"___\n",
    "   - cluster 3 (428 customers) has highest recency 319 days, lowest frequency 1 and moderate monetary value 315USD representing ___\"moderate spenders churn\"___\n",
    "   - cluster 4 (1743 customers) with 2nd highest recency 259 days, lowest frequency 1 and low monetary value 41USD represening ___\"low spenders churn\"___\n",
    "   - cluster 5 (1183 customers) low monetary value 32USD, recency 71 days and frequency 1 representing ___\"new low spenders\"___\n",
    "   - cluster 6 (1021 customers) low frequency 1 and moderate monetary value 216USD representing ___\"one time moderate spenders\"___\n",
    "   \n",
    "Both of the solutions will be considered in non-hierarchical analysis due to\n",
    " - simplicity and marketing cost efficiency of tree custers solution, where only 3 strategies need to be developed\n",
    " - ability to specify buyer personas with higher granularity, where cluster 1 typology is in center of interest representing only 83 customers who generated 35% (345K USD) of revenue.\n",
    "\n",
    "### 4.3.2 Nonhierarchical Clustering\n",
    "Nonhierarchical clustering methods have the advantage of being able to better \"optimize\" cluster solutions by reassigning observations until maximum homogeneity (similarity) is achieved. This second step in clustering process uses number of clusters (seeds) determined in hierarchical clustering. The clustering solutions are then compared to\n",
    "- how do they generalized on population\n",
    "- their predictability in respect to additional taxonomy variables (not included in this example)\n",
    "- aplicability to business objective (forming marketing strategies for different buyers personas and best ROI)\n",
    "\n",
    "#### 4.3.2.1 Seed Points Generation\n",
    "Tree and six seeds will be used respectively. No random method neither custom selection of centroids will be used. There are no priori business information about rfm characteristic of buyer personas and random method suffers from local optima issue. To cope with this sklearn __K-means++__ initialization method will be used to initialize centroids to be distant from each other, leading to provably better results than random initiallization as shown in the [reference](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf).\n",
    "\n",
    "#### 4.3.2.2 Clustering Algorithm\n",
    "__Optimization algorithm__ will be used. It allows reasignment of observation among clusters until a minimum level of heterogeneity is reached. Sklearn `KMeans` object's parameter `algorithm='auto'` will determine which algorithm branch to use according data structure. `full` for sparce data and `elkan` for dense data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-mean clustering for selected candidate solutions\n",
    "kmeans, labels = {}, {}\n",
    "for k in num_seeds:\n",
    "    \n",
    "    # Fit model\n",
    "    kmeans['non_sorted_{}'.format(k)] = (\n",
    "        KMeans(n_clusters=k, \n",
    "               random_state=random_state, \n",
    "               init='k-means++')\n",
    "        .fit(Xs['train'][idxs['transformed_std']])\n",
    "    )\n",
    "    \n",
    "    # Predict lables\n",
    "    labels['non_sorted_{}'.format(k)] = (\n",
    "        kmeans['non_sorted_{}'.format(k)]\n",
    "        .predict(Xs['train'][idxs['transformed_std']])\n",
    "    )\n",
    "    \n",
    "    Xs['train']['nonhierarchic_clusters_{}'.format(k)] = (\n",
    "        labels['non_sorted_{}'.format(k)]+1\n",
    "    )\n",
    "    \n",
    "# Display cluster sizes\n",
    "idxs['nonhierarchic_clusters'] = Xs['train'].columns[\n",
    "    Xs['train'].columns.str.contains('nonhierarchic_clusters')]\n",
    "\n",
    "for solution in idxs['nonhierarchic_clusters']:\n",
    "    display(Xs['train'].groupby(solution)\n",
    "                      .size()\n",
    "                      .rename('cluster_sizes')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test candidate solutions\n",
    "# ---------------------------\n",
    "\n",
    "# Execute cluster profile tests, print cluster profiles & scatterplots\n",
    "for solution in idxs['nonhierarchic_clusters']:\n",
    "    summary, post_hoc, prof_ax, clst_pg = (\n",
    "        helper.analyze_cluster_solution(\n",
    "            Xs['train'], idxs['transformed_std'], \n",
    "            solution, post_hoc_fnc=post_hoc_test\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Print original variable means and averages per cluster\n",
    "    str_ = 'Profile characteristics'\n",
    "    print(str_ + '\\n' + '-' * len(str_))\n",
    "\n",
    "    display(Xs['train']\n",
    "            .groupby(solution)\n",
    "            [idxs['original']]\n",
    "            .agg(['mean', 'median'])\n",
    "            .round(1)\n",
    "            .swaplevel(axis=1)\n",
    "            .sort_index(level=0, axis=1))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate within clusters homogeneity and \n",
    "# between clusters heterogeneity using Silhouette coeficient\n",
    "idxs['clusters'] = Xs['train'].columns[\n",
    "    Xs['train'].columns.str.contains('clusters')]\n",
    "\n",
    "score = pd.Series(index=idxs['clusters'], name='silhouette_score')\n",
    "\n",
    "for k, solution in zip(num_seeds * 2, idxs['clusters']):\n",
    "    score[solution] = silhouette_score(Xs['train'][idxs['transformed_std']],\n",
    "                                       Xs['train'][solution], \n",
    "                                       random_state=random_state)\n",
    "# Diplay overview\n",
    "score = score.sort_values(ascending=False)\n",
    "score.plot.bar(title = 'Silhouette coefficient')\n",
    "plt.show()\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.1 Discussion on Nonhierarchical Clustering\n",
    "Hierarchical and nonhierarchical solutions are very similary when looking to profile vizualization. The nonhierarchical solutions have ability to form more homogenous clusters with higher distictivness between clusters as it can be seen comparing clusters vizualization. This is due to the ability to reassign observations between clusters. The both 3 and 6 nonhierarchical clusters solutions have higher __Silhouette Score__ compared to their hierarchical versions.\n",
    "\n",
    "##### 4.3.2.1.1 Profiling of Cluster Solutions\n",
    "The results are very similar to hierarchical solution. They shows that there are significant diffrences between the clusters on all tree variables in both solutions. These provide initial evidence that each of the tree and six clusters is distinctive.\n",
    "\n",
    "Examination of means of cluster variables shows sligth changes in profiles:\n",
    " - tree clusters solution\n",
    "   - cluster 1 (497 customers) has highest mean on frequency 3.2 and monetary value 1129USD representing ___\"frequent shopers and high spenders\"___\n",
    "   - cluster 2 (2298 customers) with highest recency 273 days, low frequency 1, most probably representing ___\"customer churn\"___\n",
    "   - cluster 3 (2189 customers) with lowest recency 87 days, low frequency 1, represening ___\"new customers\"___ \n",
    "   \n",
    " - six clusters solution\n",
    "   - cluster 5 (83 customers) has highest mean on frequency 7.7 and monetary value 4156USD representing ___\"frequent shoppers and huge spenders\"___ most probably business customers (matching hierarchical cluster 1)\n",
    "   - cluster 3 (494 customers) has second highest mean on frequency 2.3 (no significant diffrence with cluster 1 frequency mean 7.7) and second highest monetary value 368USD representing ___\"frequent shoppers and moderate spenders\"___ (matching hierarchical cluster 2)\n",
    "   - cluster 2 (794 customers) has highest recency 294days, lowest frequency 1 and moderate monetary value 198USD representing ___\"moderate spenders churn\"___ (matching hierarchical clusters 3)\n",
    "   - cluster 6 (1478 customers) with 2nd highest recency 260 days, lowest frequency 1 and low monetary value 35USD represening ___\"low spenders churn\"___ (matching hierarchical cluster 4)\n",
    "   - cluster 1 (1490 customers) has low monetary value 39USD, recency 77 days and frequency 1 representing ___\"new low spenders\"___ (matching hierarchical cluster 5)\n",
    "   - cluster 4 (1645 customers) has low frequency 1 and moderate monetary value 310USD representing ___\"one time moderate spenders\"___ (matchin hierarchical cluster 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "The purpose of this section is to assess if generalizability of the solution to population.\n",
    "\n",
    "### 5.1 Clusters Stability\n",
    "Factors such as initial seed points, observation ordering can effect cluster membership in nonhierarchical clustering. To check stability of the clusters, dataset is sorted by monetary value and compared with original 3 and 6 cluster solutions.\n",
    "Solutions are cross tabulated for comparison of missmatched cluster membership where following rules are applied:\n",
    "\n",
    "| Cluster Stability \t| Missmatch in % \t|\n",
    "|-------------------\t| ----------------\t|\n",
    "| very stable       \t|     <0-10>     \t|\n",
    "| stable            \t|     (10-20>    \t|\n",
    "| somewhat stable   \t|     (20-25>    \t|\n",
    "| not stable        \t|       >25      \t|\n",
    "\n",
    "___Note___: Clusters numbers may not correspond to each other across diffrent algorithms and differently sorted datasets. Therefore do not expect highest numbers on diagonal of confusion matrix. Matching of the clusters need to be doublechecked vissually on scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sorted dataset by monetary value\n",
    "Xs['train_sorted'] = Xs['train'].sort_values('monetary_std').copy()\n",
    "\n",
    "# K-mean clustering for selected candidate solutions\n",
    "for k in num_seeds:\n",
    "    \n",
    "    # Fit model\n",
    "    kmeans['sorted_{}'.format(k)] = (\n",
    "        KMeans(n_clusters=k, \n",
    "               random_state=random_state, \n",
    "               init='k-means++')\n",
    "        .fit(Xs['train_sorted'][idxs['transformed_std']])\n",
    "    )\n",
    "    \n",
    "    # Predict lables\n",
    "    labels['sorted_{}'.format(k)] = (\n",
    "        kmeans['sorted_{}'.format(k)]\n",
    "        .predict(Xs['train_sorted'][idxs['transformed_std']])\n",
    "    )\n",
    "    \n",
    "    Xs['train_sorted']['nonhierarchic_sorted_clusters_{}'.format(k)] = (\n",
    "        labels['sorted_{}'.format(k)] + 1\n",
    "    )\n",
    "\n",
    "    # Cross tabulation\n",
    "    cross_tab, missmatch, total_missmatch = helper.get_missmatch(\n",
    "        index=Xs['train']['hierarchic_clusters_{}'.format(k)],\n",
    "        columns=Xs['train_sorted']['nonhierarchic_sorted_clusters_{}'.format(k)],\n",
    "        rownames=['original'],\n",
    "        colnames=['sorted']\n",
    "    )\n",
    "\n",
    "    print(cross_tab,'\\n')\n",
    "    print(missmatch, '\\n')\n",
    "    print('Total missmatch proportion: {:.2f}\\n\\n\\n'.format(total_missmatch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Clusters Stability Discussion\n",
    "The 3 and 6 cluster solution are stable as membership missmatched is > 10% (11%, 16% respectively). The 6 cluster solution showed 38% missmatched. There is a possibility to improve stability of 6 cluster solution by selecting custom centerpoints, but this would need to be supported by priori bussiness information.\n",
    "\n",
    "## 5.2 Clusters Generalizability\n",
    "Clustering methods belongs to unsupervised machine learning algorithms, so there are no ground true labels to calculate accuracy to check how model generalize to population. The diffrent approach can be used using cross tabulation between two solutions and calculating missmatch where same rules above apply. The following procedure is used: \n",
    " - Data are randomly splitted by half to `train` and `test` sample.\n",
    " - The `train` sample is used to train firt clustering model which needs to be evaluated. \n",
    " - \"True\" labels are estimated by second clustering model trained on `test` sample.  \n",
    " - Predicted labels are aquired using `KMeans.predict` function of the first fitted model\n",
    " - First model generalizability is evaluated using __confusion matrix__ cross-tabulating \"True\" and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate solution generalizability to whole population\n",
    "# ------------------------------------------------------\n",
    "for k in num_seeds:\n",
    "    \n",
    "    kmeans['train_{}'.format(k)] = kmeans['non_sorted_{}'.format(k)]\n",
    "    \n",
    "    # Fit model on test dataset\n",
    "    kmeans['test_{}'.format(k)] = (\n",
    "        KMeans(n_clusters=k, \n",
    "               random_state=random_state, \n",
    "               init='k-means++')\n",
    "        .fit(Xs['test'][idxs['transformed_std']])\n",
    "    )\n",
    "    \n",
    "    # Predict lables with model fitted on test dataset\n",
    "    labels['test_true_{}'.format(k)] = (\n",
    "        kmeans['test_{}'.format(k)]\n",
    "        .predict(Xs['test'][idxs['transformed_std']])\n",
    "    )\n",
    "    \n",
    "    Xs['test']['true_clusters_{}'.format(k)] = (\n",
    "        labels['test_true_{}'.format(k)] + 1\n",
    "    )\n",
    "\n",
    "    # Predict lables with model fitted on train dataset\n",
    "    labels['test_predicted_{}'.format(k)] = (\n",
    "        kmeans['train_{}'.format(k)]\n",
    "        .predict(Xs['test'][idxs['transformed_std']])\n",
    "    )\n",
    "    \n",
    "    Xs['test']['predicted_clusters_{}'.format(k)] = (\n",
    "        labels['test_predicted_{}'.format(k)] + 1\n",
    "    )\n",
    "    \n",
    "    # Cross tabulation\n",
    "    cross_tab, missmatch, total_missmatch = helper.get_missmatch(\n",
    "        index=Xs['test']['true_clusters_{}'.format(k)],\n",
    "        columns=Xs['test']['predicted_clusters_{}'.format(k)],\n",
    "        rownames=['true'],\n",
    "        colnames=['predicted']\n",
    "    )\n",
    "\n",
    "    print(cross_tab,'\\n')\n",
    "    print(missmatch, '\\n')\n",
    "    print('Total missmatch proportion: {:.2f}\\n\\n\\n'.format(total_missmatch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Discussion on Clusters Generalizability\n",
    "The 3 cluster solution generalize very well to population with 1% clusters membership missmatch (Accuracy Error). The 6 cluster solution generalize well with 6% error. It is acceptable to profile and compare both solutions on additional non clustering variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict cluster labels for whole dataset\n",
    "# ----------------------------------------\n",
    "\n",
    "# Standardize data\n",
    "X = rfm.drop(columns=['first_purchase'])\n",
    "X_std = scalers['train'].transform(X)\n",
    "X_std = pd.DataFrame(X_std, \n",
    "                     columns=X.columns+'_std', \n",
    "                     index=X.index)\n",
    "X = (pd.concat([X, X_std], axis=1)\n",
    "              .reindex(idxs['all_inputs'], axis=1))\n",
    "\n",
    "# Predict labels\n",
    "labels = {}\n",
    "for k in num_seeds:\n",
    "    labels[k] = (kmeans['train_{}'.format(k)]\n",
    "                 .predict(X[idxs['transformed_std']]))\n",
    "    \n",
    "    X['clusters_{}'.format(k)] = labels[k] + 1\n",
    "\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Profiling Cluster Solution on Additional Variables\n",
    "The profiling stage involves discribing the charateristics of the each buyer persona (cluster) to explain how it may differ in relevant dimensions as demograpics, consumption patterns and behaviors. These are the variables not included in cluster analysis. This section provides answer on second quastion: What are the unique characteristics of buyer personas?\n",
    "\n",
    "### 5.3.1 Data Understading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Query additional variables from google analytics dataset\n",
    "query = '''\n",
    "SELECT\n",
    "    CONCAT(fullVisitorId, CAST(visitId AS STRING)) AS session_id,\n",
    "    visitNumber AS visit_number,\n",
    "    date,\n",
    "    totals.pageviews AS pageviews,\n",
    "    totals.timeOnSite AS time_on_site,\n",
    "    trafficSource.referralPath AS referral_path,\n",
    "    trafficSource.campaign AS campaign,\n",
    "    trafficSource.source AS source,\n",
    "    trafficSource.medium AS medium,\n",
    "    trafficSource.keyword AS traffic_keyword,\n",
    "    trafficSource.adContent AS ad_content,\n",
    "    trafficSource.adwordsClickInfo.page AS ad_page,\n",
    "    trafficSource.adwordsClickInfo.slot AS ad_slot,\n",
    "    trafficSource.adwordsClickInfo.adNetworkType AS ad_network_type,\n",
    "    trafficSource.isTrueDirect AS direct_traffic,\n",
    "    device.browser AS browser,\n",
    "    device.operatingSystem AS operating_system,\n",
    "    device.deviceCategory AS device_category,\n",
    "    geoNetwork.continent AS continent,\n",
    "    geoNetwork.subContinent AS subcontinent,\n",
    "    geoNetwork.country AS country,\n",
    "    geoNetwork.region AS region,\n",
    "    geoNetwork.metro AS metro,\n",
    "    geoNetwork.city AS city,\n",
    "    geoNetwork.networkDomain AS domain,\n",
    "    customDimensions.value AS sales_region,\n",
    "    hits.hour AS hour,\n",
    "    hits.transaction.transactionId AS transaction_id,\n",
    "    hits_product.productSKU AS product_sku,\n",
    "    hits_product.v2ProductName AS product_name,\n",
    "    hits_product.v2ProductCategory AS product_category,\n",
    "    hits_product.productBrand AS product_brand,\n",
    "    (hits_product.productPrice / 1e6) AS product_price,\n",
    "    hits_product.productQuantity AS product_quantity,\n",
    "    hits.social.hasSocialSourceReferral AS is_social_referral,\n",
    "    hits.social.socialInteractionAction AS social_action,\n",
    "    hits.social.socialInteractionNetwork AS social_interaction_network,\n",
    "    hits.social.socialInteractionNetworkAction AS network_action,\n",
    "    hits.social.socialInteractions as social_interactions,\n",
    "    hits.social.socialNetwork as social_network,\n",
    "    hits.social.uniqueSocialInteractions as unique_social_interactions,\n",
    "    hits.contentGroup.contentGroup1 AS product_brand_grp,\n",
    "    hits.contentGroup.contentGroup2 AS product_category_grp,\n",
    "    fullVisitorId AS client_id,\n",
    "    channelGrouping AS channel_group\n",
    "FROM\n",
    "    `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n",
    "    LEFT JOIN UNNEST(customDimensions) AS customDimensions\n",
    "    LEFT JOIN UNNEST(hits) AS hits\n",
    "    LEFT JOIN UNNEST(hits.product) AS hits_product\n",
    "WHERE\n",
    "    _TABLE_SUFFIX BETWEEN @start_date AND @end_date\n",
    "    AND hits.transaction.transactionRevenue IS NOT NULL\n",
    "    AND hits_product.productSku IS NOT NULL\n",
    "ORDER BY\n",
    "    date\n",
    "'''\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.query_parameters = query_params\n",
    "df = client.query(query, job_config=job_config).to_dataframe()\n",
    "\n",
    "# get scale indexes\n",
    "identifiers = schema[(schema['Status'] == 'SELECTED')\n",
    "                     & (schema['Scale'] == 'IDENTIFIER')].index\n",
    "identifiers = identifiers.union(pd.Index(['session_id']))\n",
    "\n",
    "nominal = schema[(schema['Status'] == 'SELECTED')\n",
    "                 & (schema['Scale'] == 'NOMINAL')].index\n",
    "\n",
    "binary = schema[(schema['Status'] == 'SELECTED')\n",
    "                & (schema['Scale'] == 'BINARY')].index\n",
    "\n",
    "numeric = schema[(schema['Status'] == 'SELECTED')\n",
    "                 & (schema['Scale'] == 'NUMERIC')].index\n",
    "\n",
    "timestamp = schema[(schema['Status'] == 'SELECTED')\n",
    "                   & (schema['Scale'] == 'TIMESTAMP')].index\n",
    "\n",
    "# Cast variables to proper dtypes\n",
    "df[identifiers] = df[identifiers].astype(str)\n",
    "df[nominal] = df[nominal].astype('category')\n",
    "df[timestamp] = pd.to_datetime(df[timestamp].squeeze())\n",
    "\n",
    "# Prints dataframe charactersitics\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df.head())\n",
    "    \n",
    "display(df.info())\n",
    "display(df.describe(include=np.number))\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df.describe(exclude=np.number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLE MISSING VALUES\n",
    "# ---------------------\n",
    "\n",
    "# Plot missing values patterns\n",
    "nan_chars = {'nan',\n",
    "             'None',\n",
    "             '(none)',\n",
    "             '(not set)',\n",
    "             'not available in demo dataset'}\n",
    "             \n",
    "nan_mask = (df.isna() \n",
    "            | df.isnull() \n",
    "            | (df == 'nan')\n",
    "            | (df == 'None')\n",
    "            | (df == '(none)') \n",
    "            | (df == '(not set)') \n",
    "            | (df == 'not available in demo dataset'))\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = sns.heatmap(nan_mask, cbar=False);\n",
    "plt.title('Missing Data Patterns - White is missing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values proportion across variables and cases\n",
    "nan_prop_vars = ((nan_mask.sum() / nan_mask.index.size)\n",
    "                 .rename('proprotion of missing values in variables')\n",
    "                 .sort_values(ascending=False))\n",
    "\n",
    "nan_prop_cases = ((nan_mask.sum(axis=1) / nan_mask.columns.size)\n",
    "                 .rename('proprotion of missing values in cases')\n",
    "                 .sort_values(ascending=False))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "nan_prop_vars.hist(ax=ax1)\n",
    "ax1.set_title('Proportion of missing values in variables')\n",
    "nan_prop_cases.hist(ax=ax2)\n",
    "ax2.set_title('Proprotion of missing values in cases');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analyze proportion of missing values and missing vlaue codes in variables\n",
    "def analyze_nan_levels(var, nan_chars):\n",
    "    levels = var.unique()\n",
    "    nan_levels = set(levels) & nan_chars | set(levels[pd.isna(levels)])\n",
    "    return pd.Series({'num_levels': len(levels),\n",
    "                      'levels': set(levels),\n",
    "                      'num_nan_levels': len(nan_levels),\n",
    "                      'nan_levels': nan_levels})\n",
    "\n",
    "nan_vars_summary = nan_prop_vars.to_frame('nan_proportion')\n",
    "nan_vars_summary[['num_levels', \n",
    "                  'levels', \n",
    "                  'num_nan_levels', \n",
    "                  'nan_levels']] = (df.apply(analyze_nan_levels, \n",
    "                                             args=(nan_chars,))\n",
    "                                    .T)\n",
    "\n",
    "display(nan_vars_summary[nan_prop_vars > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "# -------------------\n",
    "re_engineered_vars = ['traffic_keyword',\n",
    "                      'product_brand',\n",
    "                      'product_category',\n",
    "                      'sales_region',\n",
    "                      'avg_visits',\n",
    "                      'avg_pageviews',\n",
    "                      'avg_time_on_site',\n",
    "                      'social_referral',\n",
    "                      'ad_campaign',\n",
    "                      'avg_product_revenue',\n",
    "                      'unique_products']\n",
    "\n",
    "# Reconstruct traffic variables\n",
    "# -----------------------------\n",
    "df['traffic_keyword'] = (df['traffic_keyword']\n",
    "                         .astype(str)\n",
    "                         .apply(helper.reconstruct_traffic_keyword)\n",
    "                         .astype('category'))\n",
    "\n",
    "\n",
    "# Engineer brand variable from brand keywords in product name\n",
    "# and brand variables. Remaining (not set) brand is Google brand\n",
    "# --------------------------------------------------------------\n",
    "product_df = helper.query_product_info(client, query_params)\n",
    "df['product_brand'] = (helper.reconstruct_brand(df['product_sku'], product_df)\n",
    "                       .astype('category'))\n",
    "\n",
    "# Engineer category labels from category variables and keywords in product name.\n",
    "# Remaining (not set) brand is predicted by Naive Bayes Model with \n",
    "# f1 weighted test score > 0.8% (0.985)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# load mappings from category variables to category_label\n",
    "product_category = (\n",
    "    pd.read_excel('product_categories.xlsx', \n",
    "                  sheet_name='product_category_id')\n",
    "    [['product_category', 'category_label']]\n",
    ")\n",
    "\n",
    "product_category_grp = pd.read_excel('product_categories.xlsx', \n",
    "                                        sheet_name='product_category_grp_id')\n",
    "\n",
    "category_spec = {'product_category': product_category,\n",
    "                 'product_category_grp': product_category_grp}\n",
    "\n",
    "# reconstruct category labels from category variables and product names\n",
    "df['product_category'] = (\n",
    "    helper.reconstruct_category(df['product_sku'], product_df, category_spec)\n",
    "    .astype('category')\n",
    ")\n",
    "\n",
    "# reconstruct sales region from subcontinent labels\n",
    "# -------------------------------------------------\n",
    "df['sales_region'] = (df['subcontinent']\n",
    "                       .apply(helper.reconstruct_sales_region)\n",
    "                       .astype('category'))\n",
    "\n",
    "# cast binary variable is_social_referral\n",
    "df['social_referral'] = ((df['is_social_referral']\n",
    "                          .replace(['Yes', 'No'], [True, False]))\n",
    "                          if df['is_social_referral'].dtype != 'bool'  \n",
    "                          else df['is_social_referral'])\n",
    "\n",
    "# create ad campaign flag from campaign and add variables\n",
    "ad_vars = ['campaign', 'ad_content', 'ad_page', 'ad_slot', 'ad_network_type']\n",
    "df['ad_campaign'] = False\n",
    "df.loc[~nan_mask[ad_vars].all(axis=1), 'ad_campaign'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop cases and variables with high proportion of missing values\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Variables with high proportion of missing values\n",
    "drop_vars = [\n",
    "    'referral_path',                 # not valuable information\n",
    "    'product_name',                  # too detailed, category sufficient\n",
    "    'product_brand',                 # reconstructed\n",
    "    'product_brand_grp',             # reconstructed\n",
    "    'product_category_grp',          # reconstructed\n",
    "    'is_social_referral',            # reconstructed social_referral\n",
    "    'campaign',                      # reconstructed is_ad_campaign\n",
    "    'ad_content',                    # reconstructed is_ad_campaign\n",
    "    'ad_page',                       # reconstructed is_ad_campaign\n",
    "    'ad_slot',                       # reconstructed is_ad_campaign\n",
    "    'ad_network_type',               # reconstructed is_ad_campaign\n",
    "    'domain',                        # high proportion of missing values\n",
    "    'city',                          # high proportion of missing values\n",
    "    'region',                        # high proportion of missing values\n",
    "    'metro',                         # high proportion of missing values\n",
    "    'direct_traffic',                # collinear with medium & channel group\n",
    "    'medium',                        # collinear with channel group\n",
    "    'social_action',                 # no valid values\n",
    "    'social_interaction_network',    # no valide values\n",
    "    'network_action',                # not valuable information\n",
    "    'social_interactions',           # no valide values\n",
    "    'unique_social_interactions'     # no valide values\n",
    "    \n",
    "]\n",
    "\n",
    "# cases\n",
    "cases = (nan_mask[['continent', \n",
    "                   'subcontinent', \n",
    "                   'country',\n",
    "                   'time_on_site']]\n",
    "         .any(axis=1))\n",
    "\n",
    "# delete variables and cases\n",
    "clean_df = (df.drop(columns=drop_vars, errors='ignore')\n",
    "              .drop(index=df.index[cases], errors='ignore'))\n",
    "\n",
    "# check for missing values\n",
    "clean_mask = (clean_df.isna() \n",
    "                | clean_df.isnull() \n",
    "                | (clean_df == 'nan')\n",
    "                | (clean_df == 'None')\n",
    "                | (clean_df == '(none)') \n",
    "                | (clean_df == '(not set)') \n",
    "                | (clean_df == 'not available in demo dataset'))\n",
    "\n",
    "# encode and aggregate engineered variables on customer level\n",
    "# -----------------------------------------------------------\n",
    "agg_df = helper.aggregate_data(clean_df)\n",
    "\n",
    "# dipslay summary\n",
    "txt = 'MISSING VALUE AND RE-EGINEERING SUMMARY'\n",
    "print(txt + '\\n' + '-'*len(txt) + '\\n' )\n",
    "txt = ('Missing values: {} ({:.1f}%)'\n",
    "       .format(nan_mask.sum().sum(),\n",
    "               nan_mask.sum().sum() / np.product(nan_mask.shape) * 100))       \n",
    "print(txt + '\\n')\n",
    "\n",
    "txt = ('Re-engineered variables: {} ({:.1f}%)'\n",
    "       .format(len(re_engineered_vars), \n",
    "               len(re_engineered_vars) / df.columns.size * 100))   \n",
    "print(txt)\n",
    "print(re_engineered_vars, '\\n')\n",
    "\n",
    "txt = ('Deleteted variables: {} ({:.1f}%)'\n",
    "       .format(len(drop_vars), \n",
    "               len(drop_vars) / df.columns.size * 100))       \n",
    "print(txt)\n",
    "print(drop_vars, '\\n')\n",
    "\n",
    "txt = ('Deleteted cases: {} ({:.1f}%)'\n",
    "       .format(cases.sum(), cases.sum() / df.index.size * 100))       \n",
    "print(txt)\n",
    "display(df[cases])\n",
    "\n",
    "txt = ('\\nMissing values: {} ({:.1f}%)'\n",
    "       .format(clean_df.isna().sum().sum(),\n",
    "               clean_df.isna().sum().sum() / np.prod(clean_df.shape)))       \n",
    "print(txt + '\\n')\n",
    "\n",
    "txt = ('ENCODED AND AGGREGATED DATA ON CLIENT LEVEL:')\n",
    "print(txt, '\\n' + '-' * len(txt))\n",
    "display(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add buyer personas (customer clusters) to additional data\n",
    "data = pd.merge(agg_df, \n",
    "                X[X.columns[X.columns.str.contains('cluster')]],\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
